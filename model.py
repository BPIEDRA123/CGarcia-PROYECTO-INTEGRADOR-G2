"""
model.py - Definici√≥n del modelo y utilidades de entrenamiento

Extra√≠do autom√°ticamente de Integrador31.ipynb.
Contiene funciones de creaci√≥n del modelo, entrenamiento, evaluaci√≥n y
clases auxiliares (p. ej., optimizaci√≥n de hiperpar√°metros).
"""

import os
import sys
import time
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from PIL import Image, ImageFilter, ImageEnhance
from scipy import stats, ndimage
from scipy.ndimage import sobel, gaussian_filter
from scipy.stats import kurtosis, skew, shapiro, normaltest
from scipy.stats import chi2_contingency, f_oneway
from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler, RobustScaler
from sklearn.preprocessing import OneHotEncoder, LabelEncoder, PolynomialFeatures
from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif, chi2
from sklearn.feature_selection import RFE, SelectFromModel
from sklearn.ensemble import RandomForestClassifier, IsolationForest, RandomForestRegressor
from sklearn.linear_model import LassoCV
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.model_selection import learning_curve, validation_curve, GridSearchCV, RandomizedSearchCV
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score
from sklearn.utils import class_weight, resample
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.neighbors import LocalOutlierFactor, NearestNeighbors
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.model_selection import cross_val_predict
from sklearn.inspection import PartialDependenceDisplay
import tensorflow as tf
from tensorflow.keras import layers, models, regularizers
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
from tensorflow.keras.applications import EfficientNetB0
from tensorflow.keras.optimizers import Adam, SGD, RMSprop
from tensorflow.keras.regularizers import l2, l1_l2
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from google.colab import drive, files
import warnings
from datetime import datetime
import pytz
from itertools import combinations
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots


# Constantes globales utilizadas por el modelo
SEED = 42
BASE_PATH = "/content/drive/MyDrive/p_1_image"
CLASSES = ["malignant", "benign", "normal"]
IMG_SIZE = (299, 299)
BATCH_SIZE = 32
EPOCHS = 7
VALIDATION_SPLIT = 0.15
TEST_SPLIT = 0.15
MAX_IMAGES_PER_CLASS = 10000


# Clases de modelo / optimizaci√≥n
class HyperparameterOptimizer:
    """Sistema avanzado de optimizaci√≥n de hiperpar√°metros - COMPLETAMENTE CORREGIDO"""

    def __init__(self):
        self.optimization_results = {}
        self.best_params = {}
        self.search_history = {}

    def perform_comprehensive_optimization(self, X, y, model_type='random_forest'):
        """Realizar optimizaci√≥n completa de hiperpar√°metros"""
        print(f"\nüéØ INICIANDO OPTIMIZACI√ìN COMPLETA PARA {model_type.upper()}")
        print("=" * 60)

        if model_type == 'random_forest':
            return self._optimize_random_forest(X, y)
        elif model_type == 'cnn':
            return self._optimize_cnn(X, y)
        else:
            raise ValueError(f"Tipo de modelo no soportado: {model_type}")

    def _optimize_random_forest(self, X, y):
        """Optimizaci√≥n avanzada para Random Forest"""
        print("\nüå≤ OPTIMIZANDO RANDOM FOREST CON RANDOMIZEDSEARCHCV")
        print("-" * 50)

        # Verificar que hay suficientes datos
        if len(X) < 10 or len(np.unique(y)) < 2:
            print("‚ùå No hay suficientes datos para optimizaci√≥n")
            default_params = {
                'n_estimators': 100,
                'max_depth': 10,
                'min_samples_split': 2,
                'min_samples_leaf': 1,
                'max_features': 'sqrt'
            }
            return default_params, 0.0

        # Dividir datos
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=SEED, stratify=y
        )

        # Configuraci√≥n actual
        current_config = {
            'n_estimators': 200,
            'max_depth': 15,
            'min_samples_split': 5,
            'min_samples_leaf': 2,
            'max_features': 'sqrt'
        }

        # Espacio de b√∫squeda ampliado
        param_distributions = {
            'n_estimators': [50, 100, 200, 300, 400, 500],
            'max_depth': [5, 10, 15, 20, 25, 30, None],
            'min_samples_split': [2, 5, 10, 15],
            'min_samples_leaf': [1, 2, 3, 4, 5],
            'max_features': ['sqrt', 'log2', 0.3, 0.5, 0.7],
            'bootstrap': [True, False],
            'criterion': ['gini', 'entropy']
        }

        # Configuraci√≥n actual como punto de partida
        current_model = RandomForestClassifier(**current_config, random_state=SEED)
        current_score = cross_val_score(current_model, X_train, y_train,
                                      cv=min(5, len(X_train)), scoring='accuracy').mean()

        print(f"üìä CONFIGURACI√ìN ACTUAL:")
        for param, value in current_config.items():
            print(f"   ‚Ä¢ {param}: {value}")
        print(f"   ‚Ä¢ Score actual (CV): {current_score:.4f}")

        # B√∫squeda aleatoria
        print(f"\nüîç EJECUTANDO RANDOMIZEDSEARCHCV (30 iteraciones, cv=3)...")
        start_time = time.time()

        rf_model = RandomForestClassifier(random_state=SEED)
        random_search = RandomizedSearchCV(
            rf_model,
            param_distributions=param_distributions,
            n_iter=30,
            cv=min(3, len(X_train)),
            scoring='accuracy',
            random_state=SEED,
            n_jobs=-1,
            verbose=1
        )

        random_search.fit(X_train, y_train)
        search_time = (time.time() - start_time) / 60

        # Resultados
        best_score = random_search.best_score_
        best_params = random_search.best_params_
        improvement = ((best_score - current_score) / current_score) * 100 if current_score > 0 else 0

        print(f"\n‚úÖ OPTIMIZACI√ìN COMPLETADA")
        print(f"   ‚Ä¢ Tiempo de b√∫squeda: {search_time:.2f} minutos")
        print(f"   ‚Ä¢ Mejor score encontrado: {best_score:.4f}")
        print(f"   ‚Ä¢ Mejora obtenida: {improvement:+.2f}%")

        # Almacenar resultados
        self.optimization_results['random_forest'] = {
            'current_config': current_config,
            'current_score': current_score,
            'best_params': best_params,
            'best_score': best_score,
            'improvement': improvement,
            'search_time': search_time,
            'search_results': random_search.cv_results_,
            'param_distributions': param_distributions
        }

        self.best_params['random_forest'] = best_params

        # An√°lisis posterior
        self._analyze_optimization_results(random_search, 'random_forest')

        return best_params, best_score

    def _analyze_optimization_results(self, search_cv, model_type):
        """Analizar resultados de la optimizaci√≥n"""
        print(f"\nüìà ANALIZANDO RESULTADOS DE OPTIMIZACI√ìN PARA {model_type.upper()}")

        results_df = pd.DataFrame(search_cv.cv_results_)

        # Mostrar top 5 combinaciones
        top_5 = results_df.nlargest(5, 'mean_test_score')[
            ['mean_test_score', 'std_test_score', 'params']
        ]

        print("\nüèÜ TOP 5 COMBINACIONES DE HIPERPAR√ÅMETROS:")
        for i, (idx, row) in enumerate(top_5.iterrows(), 1):
            print(f"   {i}. Score: {row['mean_test_score']:.4f} ¬± {row['std_test_score']:.4f}")
            print(f"      Par√°metros: {row['params']}")

    def _optimize_cnn(self, X, y):
        """Optimizaci√≥n para modelo CNN (implementaci√≥n b√°sica)"""
        print("\nüîÑ Optimizaci√≥n CNN - Usando par√°metros por defecto")
        best_params = {
            'learning_rate': 0.001,
            'batch_size': 32,
            'dropout_rate': 0.5,
            'optimizer': 'adam'
        }
        return best_params, 0.85

    def generate_optimization_report(self):
        """Generar reporte completo de optimizaci√≥n"""
        print("\n" + "="*80)
        print("üìã INFORME EJECUTIVO - OPTIMIZACI√ìN DE HIPERPAR√ÅMETROS")
        print("="*80)

        if not self.optimization_results:
            print("‚ùå No hay resultados de optimizaci√≥n para reportar")
            return

        for model_type, results in self.optimization_results.items():
            print(f"\nüéØ OPTIMIZACI√ìN {model_type.upper()}:")
            print(f"   ‚Ä¢ Score inicial: {results['current_score']:.4f}")
            print(f"   ‚Ä¢ Mejor score: {results['best_score']:.4f}")
            print(f"   ‚Ä¢ Mejora: {results['improvement']:+.2f}%")
            print(f"   ‚Ä¢ Tiempo de b√∫squeda: {results['search_time']:.2f} min")

            print(f"   ‚Ä¢ Mejores par√°metros encontrados:")
            for param, value in results['best_params'].items():
                print(f"     - {param}: {value}")

        return self.optimization_results


# Funciones de modelo (creaci√≥n/entrenamiento/evaluaci√≥n)
def crear_modelo_prediccion_compatible(X_features, y):
    """Funci√≥n mejorada para crear modelo de predicci√≥n - CORREGIDA"""
    caracteristicas_compatibles = [
        'intensidad_promedio', 'contraste', 'entropia', 'asimetria', 'curtosis',
        'densidad_bordes', 'magnitud_gradiente_promedio', 'hu_momento_1',
        'hu_momento_2', 'heterogeneidad'
    ]

    # Verificar que todas las caracter√≠sticas existan
    caracteristicas_disponibles = [col for col in caracteristicas_compatibles if col in X_features.columns]

    if len(caracteristicas_disponibles) < 3:
        print("‚ö†Ô∏è Muy pocas caracter√≠sticas disponibles. Usando todas las num√©ricas...")
        caracteristicas_disponibles = X_features.select_dtypes(include=[np.number]).columns.tolist()[:5]

    if not caracteristicas_disponibles:
        print("‚ùå No hay caracter√≠sticas disponibles para entrenar el modelo")
        return None, None, None, None

    X_compatible = X_features[caracteristicas_disponibles]

    le = LabelEncoder()
    y_encoded = le.fit_transform(y)

    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X_compatible)

    # Usar mejores par√°metros encontrados en la optimizaci√≥n
    model = RandomForestClassifier(
        n_estimators=200,
        max_depth=15,
        min_samples_split=5,
        min_samples_leaf=2,
        class_weight='balanced',
        random_state=SEED
    )
    model.fit(X_scaled, y_encoded)

    print(f"‚úÖ Modelo entrenado con {len(caracteristicas_disponibles)} caracter√≠sticas")
    print(f"   ‚Ä¢ Caracter√≠sticas usadas: {caracteristicas_disponibles}")

    return model, scaler, le, caracteristicas_disponibles

__all__ = ['crear_modelo_prediccion_compatible', 'HyperparameterOptimizer']
