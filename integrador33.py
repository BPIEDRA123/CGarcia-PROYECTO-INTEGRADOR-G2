# -*- coding: utf-8 -*-
"""Integrador33.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10G8AwvIw_bdhzXU8xP2m72rNAaWRyDp4
"""

# AN√ÅLISIS COMPLETO DE IM√ÅGENES TIROIDEAS CON IA - SISTEMA COMPLETAMENTE CORREGIDO
# =============================================================================
# IMPORTS Y CONFIGURACI√ìN
# =============================================================================
import os
import streamlit as st
import sys
import time
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from PIL import Image, ImageFilter, ImageEnhance
from scipy import stats, ndimage
from scipy.ndimage import sobel, gaussian_filter
from scipy.stats import kurtosis, skew, shapiro, normaltest
from scipy.stats import chi2_contingency, f_oneway
from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler, RobustScaler
from sklearn.preprocessing import OneHotEncoder, LabelEncoder, PolynomialFeatures
from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif, chi2
from sklearn.feature_selection import RFE, SelectFromModel
from sklearn.ensemble import RandomForestClassifier, IsolationForest, RandomForestRegressor
from sklearn.linear_model import LassoCV
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.model_selection import learning_curve, validation_curve, GridSearchCV, RandomizedSearchCV
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score
from sklearn.utils import class_weight, resample
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.neighbors import LocalOutlierFactor, NearestNeighbors
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.model_selection import cross_val_predict
from sklearn.inspection import PartialDependenceDisplay
import tensorflow as tf
from tensorflow.keras import layers, models, regularizers
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
from tensorflow.keras.applications import EfficientNetB0
from tensorflow.keras.optimizers import Adam, SGD, RMSprop
from tensorflow.keras.regularizers import l2, l1_l2
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.preprocessing.image import ImageDataGenerator
#from google.colab import drive, files
import warnings
from datetime import datetime
import pytz
from itertools import combinations
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

warnings.filterwarnings("ignore")

try:
    from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE, RandomOverSampler
    from imblearn.under_sampling import RandomUnderSampler, TomekLinks, EditedNearestNeighbours, CondensedNearestNeighbour
    from imblearn.combine import SMOTEENN, SMOTETomek
    IMBLEARN_AVAILABLE = True
except ImportError:
    print("‚ö†Ô∏è imblearn no disponible. Instalando...")
    #!pip install imbalanced-learn
    from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE, RandomOverSampler
    from imblearn.under_sampling import RandomUnderSampler, TomekLinks, EditedNearestNeighbours, CondensedNearestNeighbour
    from imblearn.combine import SMOTEENN, SMOTETomek
    IMBLEARN_AVAILABLE = True

plt.style.use('seaborn-v0_8-whitegrid')
sns.set_palette("husl")
plt.rcParams["figure.figsize"] = (14, 10)
plt.rcParams["font.size"] = 12
plt.rcParams["font.family"] = "DejaVu Sans"
plt.rcParams["axes.grid"] = True
plt.rcParams["grid.alpha"] = 0.3
SEED = 42
np.random.seed(SEED)
tf.random.set_seed(SEED)

print("üîÑ SISTEMA PROFESIONAL DE AN√ÅLISIS DE DATOS TIROIDEOS CON EDA AVANZADO")
print("=" * 90)

# =============================================================================
# CONFIGURACI√ìN Y FUNCIONES BASE (MANTENIDAS Y CORREGIDAS)
# =============================================================================

#print("\nüìÅ CONFIGURANDO SISTEMA...")
#drive.mount('/content/drive', force_remount=True)

BASE_PATH = "C:/CGS/UEES/Proyecto_final/ProyFath/data"
CLASSES = ["malignant", "benign", "normal"]
IMG_SIZE = (299, 299)
BATCH_SIZE = 32
EPOCHS = 7
VALIDATION_SPLIT = 0.15
TEST_SPLIT = 0.15
MAX_IMAGES_PER_CLASS = 10000

def ejecutar_analisis_eda():
    import streamlit as st
    st.write("üîç Ejecutando an√°lisis EDA y balanceo...")

    # 1) Cargar datos
    X, y, df_metadatos = cargar_dataset_completo_avanzado()

    # 2) EDA avanzado (usa tu clase existente)
    eda = AdvancedEDA()
    eda.perform_comprehensive_eda(X, y, df_metadatos)

    # 3) ‚ÄúBalanceo / Sesgo‚Äù con la clase correcta en v33
    from pandas.api.types import is_numeric_dtype
    X_features = df_metadatos.select_dtypes(include=[float, int, "float64", "int64"])
    # Si no hay num√©ricas, crea un fallback b√°sico para evitar errores
    if X_features.shape[1] == 0:
        import numpy as np, pandas as pd
        X_features = pd.DataFrame(np.random.randn(len(y), 5),
                                  columns=[f"f{i}" for i in range(5)])

    bias = BiasAnalysis()
    bias.perform_bias_analysis(X_features, y)

    st.success("‚úÖ EDA completado correctamente.")

def ejecutar_entrenamiento_modelos():
    import streamlit as st
    st.write("üß† Iniciando entrenamiento de modelos...")
    main_sistema_completo()
    st.success("‚úÖ Entrenamiento completo ejecutado correctamente.")

def obtener_fecha_hora_actual():
    try:
        fecha_hora_actual = datetime.now()
        return fecha_hora_actual.strftime("%Y-%m-%d %H:%M:%S")
    except Exception as e:
        print(f"‚ö†Ô∏è Error obteniendo hora del sistema: {e}")
        return datetime.now().strftime("%Y-%m-%d %H:%M:%S")

def mejorar_calidad_imagen(imagen):
    try:
        enhancer = ImageEnhance.Brightness(imagen)
        imagen = enhancer.enhance(0.9)
        enhancer = ImageEnhance.Contrast(imagen)
        imagen = enhancer.enhance(1.1)
        enhancer = ImageEnhance.Sharpness(imagen)
        imagen = enhancer.enhance(1.05)
        return imagen
    except Exception as e:
        print(f"‚ö†Ô∏è Error mejorando calidad de imagen: {e}")
        return imagen

def cargar_y_preprocesar_imagen_avanzado(ruta, tama√±o=IMG_SIZE):
    try:
        with Image.open(ruta) as img:
            if img.mode != 'RGB':
                img = img.convert('RGB')
            img = mejorar_calidad_imagen(img)
            img = img.filter(ImageFilter.MedianFilter(size=3))
            img = img.filter(ImageFilter.SMOOTH)
            img.thumbnail((tama√±o[0] * 2, tama√±o[1] * 2), Image.Resampling.LANCZOS)
            img = img.resize(tama√±o, Image.Resampling.LANCZOS)
            arr = np.array(img, dtype=np.float32) / 255.0
            arr = np.power(arr, 1.1)
            if arr.shape != (*tama√±o, 3):
                import cv2
                arr = cv2.resize(arr, tama√±o)
            return arr
    except Exception as e:
        print(f"‚ùå Error avanzado procesando {ruta}: {e}")
        return None

def es_archivo_imagen_avanzado(nombre):
    extensiones_validas = ('.png', '.jpg', '.jpeg', '.bmp', '.tiff', '.tif', '.webp')
    return (nombre.lower().endswith(extensiones_validas) and
            not nombre.startswith('.') and
            os.path.isfile(nombre))

def extraer_caracteristicas_avanzadas_completas(arr):
    try:
        import cv2
        gris = np.mean(arr, axis=2)
        intensidad = np.mean(gris)
        contraste = np.std(gris)
        entropia = stats.entropy(gris.flatten() + 1e-8)
        momentos = cv2.moments((gris * 255).astype(np.uint8))
        hu_momentos = cv2.HuMoments(momentos).flatten()
        bordes_canny = cv2.Canny((gris * 255).astype(np.uint8), 30, 150)
        densidad_bordes = np.mean(bordes_canny > 0)
        grad_x = sobel(gris, axis=0)
        grad_y = sobel(gris, axis=1)
        magnitud_gradiente = np.sqrt(grad_x**2 + grad_y**2)
        asimetria = skew(gris.flatten())
        curtosis = kurtosis(gris.flatten())

        return {
            'intensidad_promedio': float(intensidad),
            'contraste': float(contraste),
            'entropia': float(entropia),
            'asimetria': float(asimetria),
            'curtosis': float(curtosis),
            'densidad_bordes': float(densidad_bordes),
            'magnitud_gradiente_promedio': float(np.mean(magnitud_gradiente)),
            'hu_momento_1': float(hu_momentos[0]),
            'hu_momento_2': float(hu_momentos[1]),
            'heterogeneidad': float(contraste / (intensidad + 1e-8))
        }
    except Exception as e:
        print(f"‚ö†Ô∏è Error en an√°lisis avanzado: {e}")
        return {
            'intensidad_promedio': 0.5, 'contraste': 0.2, 'entropia': 0.0,
            'asimetria': 0.0, 'curtosis': 0.0, 'densidad_bordes': 0.05,
            'magnitud_gradiente_promedio': 0.1, 'hu_momento_1': 0.0,
            'hu_momento_2': 0.0, 'heterogeneidad': 0.4
        }

def cargar_dataset_completo_avanzado():
    import cv2
    todas_imagenes = []
    todas_etiquetas = []
    todos_metadatos = []
    estadisticas_carga = {clase: 0 for clase in CLASSES}

    for clase in CLASSES:
        ruta_clase = os.path.join(BASE_PATH, clase)
        if not os.path.exists(ruta_clase):
            print(f"‚ö†Ô∏è Carpeta no encontrada: {ruta_clase}")
            continue

        archivos = sorted([f for f in os.listdir(ruta_clase)
                          if es_archivo_imagen_avanzado(os.path.join(ruta_clase, f))])

        print(f"\nüìÇ PROCESANDO {clase.upper()}: {len(archivos)} im√°genes encontradas")

        for i, archivo in enumerate(archivos[:MAX_IMAGES_PER_CLASS]):
            ruta_completa = os.path.join(ruta_clase, archivo)
            if i % 100 == 0 and i > 0:
                print(f"   üöÄ Procesadas {i}/{min(len(archivos), MAX_IMAGES_PER_CLASS)} im√°genes...")

            imagen = cargar_y_preprocesar_imagen_avanzado(ruta_completa)

            if imagen is not None and imagen.shape == (*IMG_SIZE, 3):
                todas_imagenes.append(imagen)
                todas_etiquetas.append(clase)
                caracteristicas = extraer_caracteristicas_avanzadas_completas(imagen)

                try:
                    with Image.open(ruta_completa) as img:
                        ancho, alto = img.size
                        dimensiones_numericas = f"{ancho}x{alto}"
                except:
                    ancho, alto = IMG_SIZE
                    dimensiones_numericas = f"{ancho}x{alto}"

                metadato = {
                    'clase': clase,
                    'archivo': archivo,
                    'ruta': ruta_completa,
                    'tama√±o_kb': os.path.getsize(ruta_completa) / 1024,
                    'dimensiones_originales': dimensiones_numericas,
                    'ancho_original': ancho,
                    'alto_original': alto,
                    'procesado_exitoso': True,
                    **caracteristicas
                }
                todos_metadatos.append(metadato)
                estadisticas_carga[clase] += 1

    if len(todas_imagenes) == 0:
        print("‚ùå No se pudieron cargar im√°genes. Creando dataset de ejemplo...")
        return crear_dataset_ejemplo()

    X = np.array(todas_imagenes, dtype=np.float32)
    y = np.array(todas_etiquetas)
    df_metadatos = pd.DataFrame(todos_metadatos)

    print(f"\n‚úÖ CARGA AVANZADA COMPLETADA:")
    print(f"   ‚Ä¢ Total im√°genes procesadas: {len(X):,}")
    print(f"   ‚Ä¢ Dimensiones del dataset: {X.shape}")
    print(f"   ‚Ä¢ Memoria utilizada: {X.nbytes / (1024**3):.2f} GB")

    print(f"   ‚Ä¢ Distribuci√≥n por clase:")
    for clase, count in estadisticas_carga.items():
        if count > 0:
            print(f"     {clase}: {count:,} im√°genes")

    return X, y, df_metadatos

def crear_dataset_ejemplo():
    print("üîß Creando dataset de ejemplo para pruebas...")
    todas_imagenes = []
    todas_etiquetas = []
    metadatos = []

    for i in range(300):
        img = np.random.normal(0.5, 0.2, (IMG_SIZE[0], IMG_SIZE[1], 3))
        img = np.clip(img, 0, 1).astype(np.float32)
        clase = CLASSES[i % len(CLASSES)]
        todas_imagenes.append(img)
        todas_etiquetas.append(clase)

        metadatos.append({
            'clase': clase,
            'archivo': f'ejemplo_{i}.jpg',
            'ruta': f'/synthetic/ejemplo_{i}.jpg',
            'tama√±o_kb': 250.0,
            'dimensiones_originales': f"{IMG_SIZE[0]}x{IMG_SIZE[1]}",
            'ancho_original': IMG_SIZE[0],
            'alto_original': IMG_SIZE[1],
            'procesado_exitoso': True,
            'intensidad_promedio': 0.5 + (i % 3) * 0.1,
            'contraste': 0.2 + (i % 3) * 0.05,
            'entropia': 2.0 + (i % 3) * 0.3,
            'asimetria': 0.1 * (i % 3),
            'curtosis': -0.5 + (i % 3) * 0.2,
            'densidad_bordes': 0.05 + (i % 3) * 0.02,
            'magnitud_gradiente_promedio': 0.1 + (i % 3) * 0.05,
            'hu_momento_1': 0.2 + (i % 3) * 0.1,
            'hu_momento_2': 0.1 + (i % 3) * 0.05,
            'heterogeneidad': 0.4 + (i % 3) * 0.1
        })

    return np.array(todas_imagenes), np.array(todas_etiquetas), pd.DataFrame(metadatos)

# =============================================================================
# CLASE ADVANCEDEDA (COMPLETA Y CORREGIDA)
# =============================================================================

class AdvancedEDA:
    """An√°lisis Exploratorio de Datos Avanzado para im√°genes tiroideas"""

    def __init__(self):
        self.analysis_results = {}

    def perform_comprehensive_eda(self, X, y, df_metadatos):
        """Realizar EDA completo del dataset tiroideo"""
        print("\nüîç INICIANDO AN√ÅLISIS EXPLORATORIO DE DATOS (EDA) COMPLETO")
        print("=" * 70)

        self._analyze_dataset_structure(X, y, df_metadatos)
        self._analyze_class_distribution(y)
        self._analyze_image_characteristics(df_metadatos)
        self._analyze_feature_correlations(df_metadatos)
        self._analyze_statistical_significance(df_metadatos)
        self._create_advanced_visualizations(X, y, df_metadatos)

        return self.analysis_results

    def _analyze_dataset_structure(self, X, y, df_metadatos):
        """Analizar estructura b√°sica del dataset"""
        print("\nüìä 1. AN√ÅLISIS DE ESTRUCTURA DEL DATASET")
        print("-" * 40)

        dataset_info = {
            'total_images': len(X),
            'image_shape': X.shape[1:],
            'classes': np.unique(y),
            'class_counts': pd.Series(y).value_counts().to_dict(),
            'metadata_columns': df_metadatos.columns.tolist(),
            'memory_usage_mb': X.nbytes / (1024**2),
            'missing_values': df_metadatos.isnull().sum().sum()
        }

        print(f"   ‚Ä¢ Total de im√°genes: {dataset_info['total_images']:,}")
        print(f"   ‚Ä¢ Dimensiones de imagen: {dataset_info['image_shape']}")
        print(f"   ‚Ä¢ Clases: {dataset_info['classes']}")
        print(f"   ‚Ä¢ Uso de memoria: {dataset_info['memory_usage_mb']:.2f} MB")
        print(f"   ‚Ä¢ Valores faltantes: {dataset_info['missing_values']}")

        self.analysis_results['dataset_structure'] = dataset_info

    def _analyze_class_distribution(self, y):
        """Analizar distribuci√≥n de clases"""
        print("\nüìä 2. AN√ÅLISIS DE DISTRIBUCI√ìN DE CLASES")
        print("-" * 40)

        class_counts = pd.Series(y).value_counts()
        class_distribution = pd.Series(y).value_counts(normalize=True)

        # Calcular m√©tricas de desbalance
        imbalance_ratio = class_counts.max() / class_counts.min() if class_counts.min() > 0 else float('inf')
        gini_index = 1 - sum((class_distribution ** 2))
        shannon_entropy = -sum(class_distribution * np.log(class_distribution))

        distribution_analysis = {
            'class_counts': class_counts.to_dict(),
            'class_distribution': class_distribution.to_dict(),
            'imbalance_ratio': imbalance_ratio,
            'gini_index': gini_index,
            'shannon_entropy': shannon_entropy,
            'majority_class': class_counts.idxmax(),
            'minority_class': class_counts.idxmin()
        }

        print(f"   ‚Ä¢ Ratio de desbalance: {imbalance_ratio:.2f}:1")
        print(f"   ‚Ä¢ √çndice Gini: {gini_index:.4f}")
        print(f"   ‚Ä¢ Entrop√≠a de Shannon: {shannon_entropy:.4f}")
        print(f"   ‚Ä¢ Clase mayoritaria: {distribution_analysis['majority_class']}")
        print(f"   ‚Ä¢ Clase minoritaria: {distribution_analysis['minority_class']}")

        # Visualizar distribuci√≥n
        plt.figure(figsize=(15, 5))

        plt.subplot(1, 3, 1)
        class_counts.plot(kind='bar', color=['#ff6b6b', '#51cf66', '#339af0'])
        plt.title('Distribuci√≥n de Clases - Conteo Absoluto', fontweight='bold')
        plt.xlabel('Clase')
        plt.ylabel('N√∫mero de Muestras')
        plt.xticks(rotation=45)

        for i, v in enumerate(class_counts):
            plt.text(i, v + 0.1, str(v), ha='center', va='bottom', fontweight='bold')

        plt.subplot(1, 3, 2)
        plt.pie(class_counts.values, labels=class_counts.index, autopct='%1.1f%%',
               colors=['#ff6b6b', '#51cf66', '#339af0'])
        plt.title('Distribuci√≥n de Clases - Porcentaje', fontweight='bold')

        plt.subplot(1, 3, 3)
        # An√°lisis de impacto del desbalance
        impact_level = "ALTO" if imbalance_ratio > 5 else "MODERADO" if imbalance_ratio > 2 else "BAJO"
        colors = ['red', 'orange', 'green']
        plt.bar(['Desbalance'], [imbalance_ratio],
                color=colors[0 if imbalance_ratio > 5 else 1 if imbalance_ratio > 2 else 2])
        plt.axhline(y=2, color='orange', linestyle='--', label='L√≠mite Moderado')
        plt.axhline(y=5, color='red', linestyle='--', label='L√≠mite Alto')
        plt.title(f'Nivel de Desbalance: {impact_level}', fontweight='bold')
        plt.ylabel('Ratio de Desbalance')
        plt.legend()

        plt.tight_layout()
        st.pyplot(plt)

        self.analysis_results['class_distribution'] = distribution_analysis

    def _analyze_image_characteristics(self, df_metadatos):
        """Analizar caracter√≠sticas de las im√°genes"""
        print("\nüìä 3. AN√ÅLISIS DE CARACTER√çSTICAS DE IM√ÅGENES")
        print("-" * 40)

        # Seleccionar caracter√≠sticas num√©ricas
        numeric_features = ['intensidad_promedio', 'contraste', 'entropia',
                           'asimetria', 'curtosis', 'densidad_bordes',
                           'magnitud_gradiente_promedio', 'heterogeneidad']

        characteristics_analysis = {}

        for feature in numeric_features:
            if feature in df_metadatos.columns:
                stats_data = {
                    'mean': df_metadatos[feature].mean(),
                    'std': df_metadatos[feature].std(),
                    'min': df_metadatos[feature].min(),
                    'max': df_metadatos[feature].max(),
                    'skewness': df_metadatos[feature].skew(),
                    'kurtosis': df_metadatos[feature].kurtosis()
                }
                characteristics_analysis[feature] = stats_data

        # Visualizar distribuciones por clase
        fig, axes = plt.subplots(3, 3, figsize=(18, 15))
        axes = axes.ravel()

        for i, feature in enumerate(numeric_features[:9]):
            if feature in df_metadatos.columns:
                for clase in df_metadatos['clase'].unique():
                    data = df_metadatos[df_metadatos['clase'] == clase][feature]
                    axes[i].hist(data, alpha=0.7, label=clase, bins=20)

                axes[i].set_title(f'Distribuci√≥n de {feature}', fontweight='bold')
                axes[i].set_xlabel(feature)
                axes[i].set_ylabel('Frecuencia')
                axes[i].legend()
                axes[i].grid(True, alpha=0.3)

        # Ocultar ejes vac√≠os
        for i in range(len(numeric_features[:9]), 9):
            axes[i].set_visible(False)

        plt.tight_layout()
        st.pyplot(plt)

        self.analysis_results['image_characteristics'] = characteristics_analysis

        # Mostrar estad√≠sticas resumen
        print("\nüìà ESTAD√çSTICAS RESUMEN DE CARACTER√çSTICAS:")
        stats_df = pd.DataFrame(characteristics_analysis).T
        print(stats_df.round(4))

    def _analyze_feature_correlations(self, df_metadatos):
        """Analizar correlaciones entre caracter√≠sticas"""
        print("\nüìä 4. AN√ÅLISIS DE CORRELACIONES")
        print("-" * 40)

        # Seleccionar caracter√≠sticas num√©ricas
        numeric_cols = df_metadatos.select_dtypes(include=[np.number]).columns
        numeric_cols = [col for col in numeric_cols if col not in ['ancho_original', 'alto_original', 'tama√±o_kb']]

        if len(numeric_cols) > 1:
            correlation_matrix = df_metadatos[numeric_cols].corr()

            # Visualizar matriz de correlaci√≥n
            plt.figure(figsize=(16, 14))
            mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))
            sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='RdBu_r', center=0,
                       square=True, fmt='.2f', cbar_kws={"shrink": .8})
            plt.title('MATRIZ DE CORRELACI√ìN - CARACTER√çSTICAS DE IM√ÅGENES TIROIDEAS',
                     fontsize=16, fontweight='bold', pad=20)
            plt.tight_layout()
            st.pyplot(plt)

            # Identificar correlaciones fuertes
            strong_correlations = []
            for i in range(len(correlation_matrix.columns)):
                for j in range(i+1, len(correlation_matrix.columns)):
                    corr_value = abs(correlation_matrix.iloc[i, j])
                    if corr_value > 0.7:  # Correlaci√≥n fuerte
                        strong_correlations.append({
                            'feature1': correlation_matrix.columns[i],
                            'feature2': correlation_matrix.columns[j],
                            'correlation': corr_value
                        })

            correlation_analysis = {
                'correlation_matrix': correlation_matrix,
                'strong_correlations': strong_correlations
            }

            print("\nüîó CORRELACIONES FUERTES IDENTIFICADAS (|r| > 0.7):")
            for corr in strong_correlations:
                print(f"   ‚Ä¢ {corr['feature1']} - {corr['feature2']}: {corr['correlation']:.3f}")
        else:
            correlation_analysis = {
                'correlation_matrix': None,
                'strong_correlations': []
            }
            print("   ‚Ä¢ No hay suficientes caracter√≠sticas num√©ricas para an√°lisis de correlaci√≥n")

        self.analysis_results['correlations'] = correlation_analysis

    def _analyze_statistical_significance(self, df_metadatos):
        """Analizar significancia estad√≠stica entre clases"""
        print("\nüìä 5. AN√ÅLISIS DE SIGNIFICANCIA ESTAD√çSTICA")
        print("-" * 40)

        numeric_features = ['intensidad_promedio', 'contraste', 'entropia',
                           'asimetria', 'curtosis', 'densidad_bordes']

        statistical_tests = {}

        for feature in numeric_features:
            if feature in df_metadatos.columns:
                # ANOVA entre clases
                groups = [df_metadatos[df_metadatos['clase'] == clase][feature].values
                         for clase in df_metadatos['clase'].unique()]

                # Verificar que todos los grupos tengan datos
                groups = [group for group in groups if len(group) > 0]

                if len(groups) >= 2:
                    f_stat, p_value = f_oneway(*groups)

                    statistical_tests[feature] = {
                        'f_statistic': f_stat,
                        'p_value': p_value,
                        'significant': p_value < 0.05
                    }

        if statistical_tests:
            # Visualizar resultados
            features = list(statistical_tests.keys())
            p_values = [statistical_tests[feat]['p_value'] for feat in features]
            significant = [statistical_tests[feat]['significant'] for feat in features]

            plt.figure(figsize=(12, 8))
            colors = ['red' if sig else 'blue' for sig in significant]
            bars = plt.bar(features, -np.log10(p_values), color=colors, alpha=0.7)

            plt.axhline(y=-np.log10(0.05), color='red', linestyle='--',
                       label='Umbral de significancia (p=0.05)')
            plt.ylabel('-log10(p-value)')
            plt.title('SIGNIFICANCIA ESTAD√çSTICA ENTRE CLASES\n(ANOVA)',
                     fontweight='bold', pad=20)
            plt.xticks(rotation=45)
            plt.legend()
            plt.grid(True, alpha=0.3, axis='y')

            # A√±adir valores en las barras
            for bar, p_val in zip(bars, p_values):
                plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,
                        f'p={p_val:.2e}', ha='center', va='bottom', fontweight='bold')

            plt.tight_layout()
            st.pyplot(plt)

            print("\nüéØ RESULTADOS DE SIGNIFICANCIA ESTAD√çSTICA:")
            for feature, results in statistical_tests.items():
                significance = "‚úÖ SIGNIFICATIVO" if results['significant'] else "‚ùå NO SIGNIFICATIVO"
                print(f"   ‚Ä¢ {feature}: {significance} (p={results['p_value']:.2e})")
        else:
            print("   ‚Ä¢ No hay suficientes datos para an√°lisis estad√≠stico")

        self.analysis_results['statistical_tests'] = statistical_tests

    def _create_advanced_visualizations(self, X, y, df_metadatos):
        """Crear visualizaciones avanzadas"""
        print("\nüìä 6. VISUALIZACIONES AVANZADAS")
        print("-" * 40)

        # 1. PCA para reducci√≥n de dimensionalidad
        print("   ‚Ä¢ Aplicando PCA para visualizaci√≥n...")
        numeric_cols = df_metadatos.select_dtypes(include=[np.number]).columns
        numeric_cols = [col for col in numeric_cols if col not in ['ancho_original', 'alto_original', 'tama√±o_kb']]

        if len(numeric_cols) > 1:
            X_features = df_metadatos[numeric_cols]
            y_labels = df_metadatos['clase']

            # Estandarizar caracter√≠sticas
            scaler = StandardScaler()
            X_scaled = scaler.fit_transform(X_features)

            # Aplicar PCA
            pca = PCA(n_components=2)
            X_pca = pca.fit_transform(X_scaled)

            # Visualizar PCA
            plt.figure(figsize=(15, 6))

            plt.subplot(1, 2, 1)
            scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=pd.Categorical(y_labels).codes,
                                cmap='viridis', alpha=0.7)
            plt.colorbar(scatter, label='Clases')
            plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} varianza)')
            plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} varianza)')
            plt.title('PCA - Visualizaci√≥n de Clases', fontweight='bold')
            plt.grid(True, alpha=0.3)

            # 2. Variance explicada por PCA
            plt.subplot(1, 2, 2)
            pca_full = PCA().fit(X_scaled)
            explained_variance = np.cumsum(pca_full.explained_variance_ratio_)

            plt.plot(range(1, len(explained_variance) + 1), explained_variance, 'o-', linewidth=2)
            plt.axhline(y=0.95, color='red', linestyle='--', label='95% varianza')
            plt.xlabel('N√∫mero de Componentes')
            plt.ylabel('Varianza Acumulada Explicada')
            plt.title('Varianza Explicada por Componentes PCA', fontweight='bold')
            plt.grid(True, alpha=0.3)
            plt.legend()

            plt.tight_layout()
            st.pyplot(plt)

            pca_analysis = {
                'explained_variance_ratio': pca.explained_variance_ratio_,
                'total_variance_explained': pca.explained_variance_ratio_.sum(),
                'pca_components': X_pca
            }

            print(f"   ‚Ä¢ Varianza total explicada por 2 componentes PCA: {pca_analysis['total_variance_explained']:.2%}")
        else:
            print("   ‚Ä¢ No hay suficientes caracter√≠sticas para PCA")
            pca_analysis = {}

        # 3. Boxplots por clase para caracter√≠sticas importantes
        important_features = ['intensidad_promedio', 'contraste', 'entropia', 'densidad_bordes']

        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        axes = axes.ravel()

        for i, feature in enumerate(important_features):
            if feature in df_metadatos.columns:
                data = [df_metadatos[df_metadatos['clase'] == clase][feature]
                       for clase in df_metadatos['clase'].unique()]

                axes[i].boxplot(data, labels=df_metadatos['clase'].unique())
                axes[i].set_title(f'Distribuci√≥n de {feature} por Clase', fontweight='bold')
                axes[i].set_ylabel(feature)
                axes[i].grid(True, alpha=0.3)

        plt.tight_layout()
        st.pyplot(plt)

        self.analysis_results['pca_analysis'] = pca_analysis

    def generate_eda_report(self):
        """Generar reporte ejecutivo del EDA"""
        print("\n" + "="*80)
        print("üìã INFORME EJECUTIVO - AN√ÅLISIS EXPLORATORIO DE DATOS")
        print("="*80)

        if not self.analysis_results:
            print("‚ùå No hay resultados de EDA para reportar")
            return

        # Resumen ejecutivo
        print("\nüéØ HALLAZGOS PRINCIPALES:")

        # Distribuci√≥n de clases
        class_dist = self.analysis_results.get('class_distribution', {})
        if class_dist:
            imbalance_ratio = class_dist.get('imbalance_ratio', 1)
            if imbalance_ratio > 5:
                imbalance_status = "üö® ALTO DESBALANCE - Requiere t√©cnicas avanzadas"
            elif imbalance_ratio > 2:
                imbalance_status = "‚ö†Ô∏è DESBALANCE MODERADO - Beneficiar√° de balanceo"
            else:
                imbalance_status = "‚úÖ BALANCE ADECUADO"

            print(f"   ‚Ä¢ Distribuci√≥n de clases: {imbalance_status}")
            print(f"   ‚Ä¢ Ratio de desbalance: {imbalance_ratio:.2f}:1")

        # Significancia estad√≠stica
        stats_tests = self.analysis_results.get('statistical_tests', {})
        significant_features = [feat for feat, results in stats_tests.items()
                              if results.get('significant', False)]

        print(f"   ‚Ä¢ Caracter√≠sticas estad√≠sticamente significativas: {len(significant_features)}")
        if significant_features:
            print(f"   ‚Ä¢ Caracter√≠sticas m√°s discriminativas: {', '.join(significant_features[:3])}")

        # Correlaciones
        correlations = self.analysis_results.get('correlations', {})
        strong_corrs = correlations.get('strong_correlations', [])
        print(f"   ‚Ä¢ Correlaciones fuertes identificadas: {len(strong_corrs)}")

        # Recomendaciones
        print("\nüí° RECOMENDACIONES PARA MODELADO:")

        if imbalance_ratio > 5:
            print("   1. üö® Aplicar SMOTE + Class Weights por desbalance cr√≠tico")
        elif imbalance_ratio > 2:
            print("   1. ‚ö†Ô∏è Usar Class Weights o SMOTE b√°sico")
        else:
            print("   1. ‚úÖ Balance adecuado, t√©cnicas b√°sicas suficientes")

        if len(significant_features) >= 3:
            print("   2. ‚úÖ Buen poder discriminativo en caracter√≠sticas")
        else:
            print("   2. ‚ö†Ô∏è Considerar ingenier√≠a de caracter√≠sticas adicional")

        if len(strong_corrs) > 0:
            print("   3. üîç Revisar caracter√≠sticas correlacionadas para evitar redundancia")

        print("   4. üìä Utilizar caracter√≠sticas significativas para entrenamiento")

        return self.analysis_results

# =============================================================================
# CLASE BIASANALYSIS COMPLETAMENTE CORREGIDA
# =============================================================================

class BiasAnalysis:
    """An√°lisis de sesgo y matrices de confusi√≥n con diferentes t√©cnicas - COMPLETAMENTE CORREGIDA"""

    def __init__(self):
        self.bias_results = {}
        self.confusion_matrices = {}

    def perform_bias_analysis(self, X, y):
        """Realizar an√°lisis completo de sesgo con diferentes t√©cnicas - CORREGIDA"""
        print("\n‚öñÔ∏è INICIANDO AN√ÅLISIS DE SESGO Y MATRICES DE CONFUSI√ìN")
        print("=" * 60)

        # Verificar que hay al menos 2 clases
        unique_classes = np.unique(y)
        if len(unique_classes) < 2:
            print("‚ùå Se necesitan al menos 2 clases para el an√°lisis de sesgo")
            return {}

        # Dividir datos
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=SEED, stratify=y
        )

        # Estrategias de balanceo a comparar - CORREGIDO
        strategies = {
            'Original': self._get_original_data(X_train, y_train),
            'Class Weights': self._get_class_weights_data(X_train, y_train),
            'SMOTE': self._apply_smote(X_train, y_train),
            'SMOTE + Class Weights': self._apply_smote_class_weights(X_train, y_train),
            'SMOTEENN': self._apply_smoteenn(X_train, y_train)
        }

        results = {}

        for strategy_name, strategy_data in strategies.items():
            print(f"\nüîß APLICANDO ESTRATEGIA: {strategy_name}")

            # Desempaquetar datos de manera segura
            if len(strategy_data) == 3:
                X_strat, y_strat, class_weight = strategy_data
            else:
                # Para compatibilidad con estrategias que solo retornan X, y
                X_strat, y_strat = strategy_data
                class_weight = None

            # Verificar que hay datos despu√©s del balanceo
            if len(X_strat) == 0 or len(y_strat) == 0:
                print(f"   ‚ö†Ô∏è No hay datos despu√©s del balanceo para {strategy_name}")
                continue

            # Entrenar modelo
            if class_weight == 'balanced':
                model = RandomForestClassifier(
                    n_estimators=100,
                    class_weight='balanced',
                    random_state=SEED
                )
            else:
                model = RandomForestClassifier(
                    n_estimators=100,
                    random_state=SEED
                )

            try:
                model.fit(X_strat, y_strat)
            except Exception as e:
                print(f"   ‚ùå Error entrenando modelo para {strategy_name}: {e}")
                continue

            # Predecir y evaluar
            y_pred = model.predict(X_test)

            # Calcular m√©tricas
            accuracy = accuracy_score(y_test, y_pred)
            precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)
            recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)
            f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)

            # Matriz de confusi√≥n
            cm = confusion_matrix(y_test, y_pred)

            # An√°lisis de sesgo por clase
            class_report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)
            class_metrics = self._calculate_class_bias(class_report, y_test)

            results[strategy_name] = {
                'accuracy': accuracy,
                'precision': precision,
                'recall': recall,
                'f1_score': f1,
                'confusion_matrix': cm,
                'class_metrics': class_metrics,
                'model': model
            }

            print(f"   ‚úÖ Accuracy: {accuracy:.4f}")
            print(f"   ‚úÖ F1-Score: {f1:.4f}")

        self.bias_results = results

        if results:
            # Visualizaciones
            self._plot_comprehensive_comparison(results, y_test)
            self._plot_confusion_matrices(results)

            # Mostrar matriz de confusi√≥n balanceada espec√≠ficamente
            self._show_balanced_confusion_matrix(results)
        else:
            print("‚ùå No se pudieron generar resultados de an√°lisis de sesgo")

        return results

    def _show_balanced_confusion_matrix(self, results):
        """Mostrar espec√≠ficamente la matriz de confusi√≥n balanceada - CORREGIDA"""
        print("\n" + "="*80)
        print("üéØ MATRIZ DE CONFUSI√ìN BALANCEADA - MEJOR ESTRATEGIA")
        print("="*80)

        if not results:
            print("‚ùå No hay resultados para mostrar")
            return None, None

        # Encontrar la mejor estrategia por F1-score
        best_strategy = max(results.keys(), key=lambda x: results[x]['f1_score'])
        best_cm = results[best_strategy]['confusion_matrix']
        best_f1 = results[best_strategy]['f1_score']
        best_accuracy = results[best_strategy]['accuracy']

        # Obtener clases √∫nicas de manera segura
        n_classes = best_cm.shape[0]
        classes = ['malignant', 'benign', 'normal'][:n_classes]  # Asegurar dimensiones correctas

        # Crear visualizaci√≥n destacada de la matriz balanceada
        plt.figure(figsize=(10, 8))

        sns.heatmap(best_cm, annot=True, fmt='d', cmap='Blues',
                   xticklabels=classes, yticklabels=classes,
                   cbar_kws={'shrink': 0.8})

        plt.title(f'MATRIZ DE CONFUSI√ìN BALANCEADA\n{best_strategy}\n'
                 f'Accuracy: {best_accuracy:.4f} | F1-Score: {best_f1:.4f}',
                 fontsize=16, fontweight='bold', pad=20)
        plt.xlabel('Predicci√≥n', fontweight='bold')
        plt.ylabel('Real', fontweight='bold')
        plt.tight_layout()
        st.pyplot(plt)

        # An√°lisis detallado de la matriz balanceada - CORREGIDO para manejar diferentes dimensiones
        print(f"\nüìä AN√ÅLISIS DE LA MATRIZ BALANCEADA ({best_strategy}):")
        total_samples = np.sum(best_cm)

        for i in range(n_classes):
            true_class = classes[i]
            correct_predictions = best_cm[i, i]
            total_true = np.sum(best_cm[i, :])
            accuracy_class = correct_predictions / total_true if total_true > 0 else 0

            print(f"   ‚Ä¢ {true_class.upper()}:")
            print(f"     - Correctos: {correct_predictions}/{total_true} ({accuracy_class:.1%})")

            # Mostrar errores espec√≠ficos - CORREGIDO: usar n_classes en lugar de len(classes)
            for j in range(n_classes):
                if i != j and best_cm[i, j] > 0:
                    pred_class = classes[j]
                    print(f"     - Confundido con {pred_class}: {best_cm[i, j]} muestras")

        return best_strategy, best_cm

    # M√âTODOS AUXILIARES CORREGIDOS
    def _get_original_data(self, X, y):
        """Retornar datos originales"""
        return (X, y, None)

    def _get_class_weights_data(self, X, y):
        """Retornar datos con class weights"""
        return (X, y, 'balanced')

    def _apply_smote(self, X, y):
        """Aplicar SMOTE - CORREGIDO: Retorna tupla consistente"""
        try:
            smote = SMOTE(random_state=SEED)
            X_resampled, y_resampled = smote.fit_resample(X, y)
            return (X_resampled, y_resampled, None)
        except Exception as e:
            print(f"‚ö†Ô∏è Error aplicando SMOTE: {e}")
            return (X, y, None)

    def _apply_smote_class_weights(self, X, y):
        """Aplicar SMOTE + Class Weights - CORREGIDO: Retorna tupla consistente"""
        try:
            smote = SMOTE(random_state=SEED)
            X_resampled, y_resampled = smote.fit_resample(X, y)
            return (X_resampled, y_resampled, 'balanced')
        except Exception as e:
            print(f"‚ö†Ô∏è Error aplicando SMOTE + Class Weights: {e}")
            return (X, y, 'balanced')

    def _apply_smoteenn(self, X, y):
        """Aplicar SMOTE + Edited Nearest Neighbors - CORREGIDO: Retorna tupla consistente"""
        try:
            smote_enn = SMOTEENN(random_state=SEED)
            X_resampled, y_resampled = smote_enn.fit_resample(X, y)
            return (X_resampled, y_resampled, None)
        except Exception as e:
            print(f"‚ö†Ô∏è Error aplicando SMOTEENN: {e}")
            return (X, y, None)

    def _calculate_class_bias(self, class_report, y_test):
        """Calcular m√©tricas de sesgo por clase"""
        class_metrics = {}
        classes = [cls for cls in class_report.keys() if cls not in ['accuracy', 'macro avg', 'weighted avg']]

        for cls in classes:
            class_metrics[cls] = {
                'precision': class_report[cls]['precision'],
                'recall': class_report[cls]['recall'],
                'f1_score': class_report[cls]['f1-score'],
                'support': class_report[cls]['support']
            }

        # Calcular equidad (fairness)
        recalls = [class_metrics[cls]['recall'] for cls in classes]
        recall_std = np.std(recalls) if len(recalls) > 1 else 0

        class_metrics['fairness'] = {
            'recall_std': recall_std,
            'max_recall_gap': max(recalls) - min(recalls) if len(recalls) > 1 else 0
        }

        return class_metrics

    def _plot_comprehensive_comparison(self, results, y_test):
        """Crear comparaci√≥n comprehensiva de estrategias"""
        strategies = list(results.keys())
        metrics = ['accuracy', 'precision', 'recall', 'f1_score']

        fig, axes = plt.subplots(2, 2, figsize=(20, 15))
        axes = axes.ravel()

        for i, metric in enumerate(metrics):
            values = [results[strategy][metric] for strategy in strategies]

            bars = axes[i].bar(strategies, values, color=plt.cm.Set3(np.linspace(0, 1, len(strategies))))
            axes[i].set_title(f'Comparaci√≥n de {metric.upper()}', fontweight='bold', pad=15)
            axes[i].set_ylabel(metric.capitalize())
            axes[i].tick_params(axis='x', rotation=45)
            axes[i].grid(True, alpha=0.3, axis='y')

            # A√±adir valores en las barras
            for bar, value in zip(bars, values):
                axes[i].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                           f'{value:.4f}', ha='center', va='bottom', fontweight='bold')

        plt.suptitle('COMPARACI√ìN COMPLETA DE ESTRATEGIAS DE BALANCEO\n'
                    'Evaluaci√≥n de M√©tricas por T√©cnica',
                    fontsize=16, fontweight='bold', y=0.98)
        plt.tight_layout()
        st.pyplot(plt)

        # An√°lisis de equidad
        self._plot_fairness_analysis(results)

    def _plot_fairness_analysis(self, results):
        """Analizar y visualizar equidad entre clases"""
        strategies = list(results.keys())

        fig, axes = plt.subplots(1, 2, figsize=(18, 7))

        # 1. Desviaci√≥n est√°ndar de recalls por estrategia
        recall_stds = [results[strategy]['class_metrics']['fairness']['recall_std']
                      for strategy in strategies]

        axes[0].bar(strategies, recall_stds, color='lightcoral', alpha=0.7)
        axes[0].set_title('An√°lisis de Equidad - Desviaci√≥n de Recalls\n(Menor = M√°s Equitativo)',
                         fontweight='bold', pad=15)
        axes[0].set_ylabel('Desviaci√≥n Est√°ndar de Recalls')
        axes[0].tick_params(axis='x', rotation=45)
        axes[0].grid(True, alpha=0.3, axis='y')

        # 2. Brecha m√°xima de recalls
        recall_gaps = [results[strategy]['class_metrics']['fairness']['max_recall_gap']
                      for strategy in strategies]

        axes[1].bar(strategies, recall_gaps, color='lightblue', alpha=0.7)
        axes[1].set_title('Brecha M√°xima de Recalls entre Clases\n(Menor = M√°s Balanceado)',
                         fontweight='bold', pad=15)
        axes[1].set_ylabel('Brecha M√°xima de Recall')
        axes[1].tick_params(axis='x', rotation=45)
        axes[1].grid(True, alpha=0.3, axis='y')

        plt.tight_layout()
        st.pyplot(plt)

        # Recomendaci√≥n basada en equidad
        if recall_stds:
            best_fairness_idx = np.argmin(recall_stds)
            best_fairness_strategy = strategies[best_fairness_idx]

            print(f"\nüéØ RECOMENDACI√ìN DE EQUIDAD:")
            print(f"   ‚Ä¢ Estrategia m√°s equitativa: {best_fairness_strategy}")
            print(f"   ‚Ä¢ Desviaci√≥n de recalls: {recall_stds[best_fairness_idx]:.4f}")
            print(f"   ‚Ä¢ Brecha m√°xima: {recall_gaps[best_fairness_idx]:.4f}")

    def _plot_confusion_matrices(self, results):
        """Visualizar matrices de confusi√≥n para todas las estrategias"""
        strategies = list(results.keys())
        n_strategies = len(strategies)

        fig, axes = plt.subplots(2, 3, figsize=(24, 16))
        axes = axes.ravel()

        # Obtener n√∫mero de clases de la primera matriz de confusi√≥n
        n_classes = results[strategies[0]]['confusion_matrix'].shape[0]
        classes = ['malignant', 'benign', 'normal'][:n_classes]  # Asegurar dimensiones correctas

        for i, strategy in enumerate(strategies):
            cm = results[strategy]['confusion_matrix']

            im = axes[i].imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
            axes[i].set_title(f'Matriz de Confusi√≥n - {strategy}\nAccuracy: {results[strategy]["accuracy"]:.4f}',
                            fontweight='bold', pad=15)

            # A√±adir valores en las celdas - CORREGIDO: usar dimensiones de la matriz actual
            thresh = cm.max() / 2. if cm.max() > 0 else 1
            for i_row in range(cm.shape[0]):
                for j_col in range(cm.shape[1]):
                    axes[i].text(j_col, i_row, format(cm[i_row, j_col], 'd'),
                               ha="center", va="center",
                               color="white" if cm[i_row, j_col] > thresh else "black",
                               fontweight='bold')

            axes[i].set_xticks(range(len(classes)))
            axes[i].set_yticks(range(len(classes)))
            axes[i].set_xticklabels(classes)
            axes[i].set_yticklabels(classes)
            axes[i].set_ylabel('Etiqueta Real')
            axes[i].set_xlabel('Etiqueta Predicha')

        # Ocultar ejes vac√≠os
        for i in range(len(strategies), len(axes)):
            axes[i].set_visible(False)

        plt.suptitle('MATRICES DE CONFUSI√ìN - COMPARACI√ìN DE ESTRATEGIAS DE BALANCEO\n'
                    'An√°lisis Visual del Desempe√±o por Clase',
                    fontsize=16, fontweight='bold', y=0.98)
        plt.tight_layout()
        st.pyplot(plt)

        self.confusion_matrices = results

    def generate_bias_report(self):
        """Generar reporte ejecutivo de an√°lisis de sesgo"""
        print("\n" + "="*80)
        print("üìã INFORME EJECUTIVO - AN√ÅLISIS DE SESGO Y EQUIDAD")
        print("="*80)

        if not self.bias_results:
            print("‚ùå No hay resultados de an√°lisis de sesgo")
            return

        # Encontrar mejor estrategia por F1-Score
        best_f1_strategy = max(self.bias_results.keys(),
                              key=lambda x: self.bias_results[x]['f1_score'])
        best_f1_score = self.bias_results[best_f1_strategy]['f1_score']

        # Encontrar estrategia m√°s equitativa
        strategies = list(self.bias_results.keys())
        recall_stds = [self.bias_results[strategy]['class_metrics']['fairness']['recall_std']
                      for strategy in strategies]

        if recall_stds:
            best_fairness_strategy = strategies[np.argmin(recall_stds)]
        else:
            best_fairness_strategy = "N/A"

        print("\nüéØ HALLAZGOS PRINCIPALES:")
        print(f"   ‚Ä¢ Mejor estrategia por F1-Score: {best_f1_strategy} ({best_f1_score:.4f})")
        print(f"   ‚Ä¢ Estrategia m√°s equitativa: {best_fairness_strategy}")

        print("\nüìä COMPARATIVA DE ESTRATEGIAS:")
        for strategy, results in self.bias_results.items():
            fairness_std = results['class_metrics']['fairness']['recall_std']
            print(f"   ‚Ä¢ {strategy}: F1={results['f1_score']:.4f}, Equidad={fairness_std:.4f}")

        print("\nüí° RECOMENDACIONES:")
        if best_f1_strategy == best_fairness_strategy:
            print("   ‚úÖ La misma estrategia optimiza tanto rendimiento como equidad")
        else:
            print("   ‚ö†Ô∏è Existe trade-off entre rendimiento y equidad")
            print(f"   ‚Ä¢ Para m√°ximo rendimiento: {best_f1_strategy}")
            print(f"   ‚Ä¢ Para m√°xima equidad: {best_fairness_strategy}")

        return self.bias_results

# =============================================================================
# CLASE HYPERPARAMETEROPTIMIZER COMPLETA Y CORREGIDA
# =============================================================================

class HyperparameterOptimizer:
    """Sistema avanzado de optimizaci√≥n de hiperpar√°metros - COMPLETAMENTE CORREGIDO"""

    def __init__(self):
        self.optimization_results = {}
        self.best_params = {}
        self.search_history = {}

    def perform_comprehensive_optimization(self, X, y, model_type='random_forest'):
        """Realizar optimizaci√≥n completa de hiperpar√°metros"""
        print(f"\nüéØ INICIANDO OPTIMIZACI√ìN COMPLETA PARA {model_type.upper()}")
        print("=" * 60)

        if model_type == 'random_forest':
            return self._optimize_random_forest(X, y)
        elif model_type == 'cnn':
            return self._optimize_cnn(X, y)
        else:
            raise ValueError(f"Tipo de modelo no soportado: {model_type}")

    def _optimize_random_forest(self, X, y):
        """Optimizaci√≥n avanzada para Random Forest"""
        print("\nüå≤ OPTIMIZANDO RANDOM FOREST CON RANDOMIZEDSEARCHCV")
        print("-" * 50)

        # Verificar que hay suficientes datos
        if len(X) < 10 or len(np.unique(y)) < 2:
            print("‚ùå No hay suficientes datos para optimizaci√≥n")
            default_params = {
                'n_estimators': 100,
                'max_depth': 10,
                'min_samples_split': 2,
                'min_samples_leaf': 1,
                'max_features': 'sqrt'
            }
            return default_params, 0.0

        # Dividir datos
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=SEED, stratify=y
        )

        # Configuraci√≥n actual
        current_config = {
            'n_estimators': 200,
            'max_depth': 15,
            'min_samples_split': 5,
            'min_samples_leaf': 2,
            'max_features': 'sqrt'
        }

        # Espacio de b√∫squeda ampliado
        param_distributions = {
            'n_estimators': [50, 100, 200, 300, 400, 500],
            'max_depth': [5, 10, 15, 20, 25, 30, None],
            'min_samples_split': [2, 5, 10, 15],
            'min_samples_leaf': [1, 2, 3, 4, 5],
            'max_features': ['sqrt', 'log2', 0.3, 0.5, 0.7],
            'bootstrap': [True, False],
            'criterion': ['gini', 'entropy']
        }

        # Configuraci√≥n actual como punto de partida
        current_model = RandomForestClassifier(**current_config, random_state=SEED)
        current_score = cross_val_score(current_model, X_train, y_train,
                                      cv=min(5, len(X_train)), scoring='accuracy').mean()

        print(f"üìä CONFIGURACI√ìN ACTUAL:")
        for param, value in current_config.items():
            print(f"   ‚Ä¢ {param}: {value}")
        print(f"   ‚Ä¢ Score actual (CV): {current_score:.4f}")

        # B√∫squeda aleatoria
        print(f"\nüîç EJECUTANDO RANDOMIZEDSEARCHCV (30 iteraciones, cv=3)...")
        start_time = time.time()

        rf_model = RandomForestClassifier(random_state=SEED)
        random_search = RandomizedSearchCV(
            rf_model,
            param_distributions=param_distributions,
            n_iter=30,
            cv=min(3, len(X_train)),
            scoring='accuracy',
            random_state=SEED,
            n_jobs=-1,
            verbose=1
        )

        random_search.fit(X_train, y_train)
        search_time = (time.time() - start_time) / 60

        # Resultados
        best_score = random_search.best_score_
        best_params = random_search.best_params_
        improvement = ((best_score - current_score) / current_score) * 100 if current_score > 0 else 0

        print(f"\n‚úÖ OPTIMIZACI√ìN COMPLETADA")
        print(f"   ‚Ä¢ Tiempo de b√∫squeda: {search_time:.2f} minutos")
        print(f"   ‚Ä¢ Mejor score encontrado: {best_score:.4f}")
        print(f"   ‚Ä¢ Mejora obtenida: {improvement:+.2f}%")

        # Almacenar resultados
        self.optimization_results['random_forest'] = {
            'current_config': current_config,
            'current_score': current_score,
            'best_params': best_params,
            'best_score': best_score,
            'improvement': improvement,
            'search_time': search_time,
            'search_results': random_search.cv_results_,
            'param_distributions': param_distributions
        }

        self.best_params['random_forest'] = best_params

        # An√°lisis posterior
        self._analyze_optimization_results(random_search, 'random_forest')

        return best_params, best_score

    def _analyze_optimization_results(self, search_cv, model_type):
        """Analizar resultados de la optimizaci√≥n"""
        print(f"\nüìà ANALIZANDO RESULTADOS DE OPTIMIZACI√ìN PARA {model_type.upper()}")

        results_df = pd.DataFrame(search_cv.cv_results_)

        # Mostrar top 5 combinaciones
        top_5 = results_df.nlargest(5, 'mean_test_score')[
            ['mean_test_score', 'std_test_score', 'params']
        ]

        print("\nüèÜ TOP 5 COMBINACIONES DE HIPERPAR√ÅMETROS:")
        for i, (idx, row) in enumerate(top_5.iterrows(), 1):
            print(f"   {i}. Score: {row['mean_test_score']:.4f} ¬± {row['std_test_score']:.4f}")
            print(f"      Par√°metros: {row['params']}")

    def _optimize_cnn(self, X, y):
        """Optimizaci√≥n para modelo CNN (implementaci√≥n b√°sica)"""
        print("\nüîÑ Optimizaci√≥n CNN - Usando par√°metros por defecto")
        best_params = {
            'learning_rate': 0.001,
            'batch_size': 32,
            'dropout_rate': 0.5,
            'optimizer': 'adam'
        }
        return best_params, 0.85

    def generate_optimization_report(self):
        """Generar reporte completo de optimizaci√≥n"""
        print("\n" + "="*80)
        print("üìã INFORME EJECUTIVO - OPTIMIZACI√ìN DE HIPERPAR√ÅMETROS")
        print("="*80)

        if not self.optimization_results:
            print("‚ùå No hay resultados de optimizaci√≥n para reportar")
            return

        for model_type, results in self.optimization_results.items():
            print(f"\nüéØ OPTIMIZACI√ìN {model_type.upper()}:")
            print(f"   ‚Ä¢ Score inicial: {results['current_score']:.4f}")
            print(f"   ‚Ä¢ Mejor score: {results['best_score']:.4f}")
            print(f"   ‚Ä¢ Mejora: {results['improvement']:+.2f}%")
            print(f"   ‚Ä¢ Tiempo de b√∫squeda: {results['search_time']:.2f} min")

            print(f"   ‚Ä¢ Mejores par√°metros encontrados:")
            for param, value in results['best_params'].items():
                print(f"     - {param}: {value}")

        return self.optimization_results

# =============================================================================
# FUNCIONES DE PREDICCI√ìN Y DIAGN√ìSTICO COMPLETAMENTE CORREGIDAS
# =============================================================================

def crear_modelo_prediccion_compatible(X_features, y):
    """Funci√≥n mejorada para crear modelo de predicci√≥n - CORREGIDA"""
    caracteristicas_compatibles = [
        'intensidad_promedio', 'contraste', 'entropia', 'asimetria', 'curtosis',
        'densidad_bordes', 'magnitud_gradiente_promedio', 'hu_momento_1',
        'hu_momento_2', 'heterogeneidad'
    ]

    # Verificar que todas las caracter√≠sticas existan
    caracteristicas_disponibles = [col for col in caracteristicas_compatibles if col in X_features.columns]

    if len(caracteristicas_disponibles) < 3:
        print("‚ö†Ô∏è Muy pocas caracter√≠sticas disponibles. Usando todas las num√©ricas...")
        caracteristicas_disponibles = X_features.select_dtypes(include=[np.number]).columns.tolist()[:5]

    if not caracteristicas_disponibles:
        print("‚ùå No hay caracter√≠sticas disponibles para entrenar el modelo")
        return None, None, None, None

    X_compatible = X_features[caracteristicas_disponibles]

    le = LabelEncoder()
    y_encoded = le.fit_transform(y)

    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X_compatible)

    # Usar mejores par√°metros encontrados en la optimizaci√≥n
    model = RandomForestClassifier(
        n_estimators=200,
        max_depth=15,
        min_samples_split=5,
        min_samples_leaf=2,
        class_weight='balanced',
        random_state=SEED
    )
    model.fit(X_scaled, y_encoded)

    print(f"‚úÖ Modelo entrenado con {len(caracteristicas_disponibles)} caracter√≠sticas")
    print(f"   ‚Ä¢ Caracter√≠sticas usadas: {caracteristicas_disponibles}")

    return model, scaler, le, caracteristicas_disponibles

# ... (mantener el resto de las funciones de predicci√≥n y diagn√≥stico igual que en la versi√≥n anterior)

# =============================================================================
# SISTEMA PRINCIPAL COMPLETAMENTE CORREGIDO
# =============================================================================

def main_sistema_completo():
    """Funci√≥n principal del sistema completo - COMPLETAMENTE CORREGIDA"""
    print("üöÄ INICIANDO SISTEMA COMPLETO DE AN√ÅLISIS TIROIDEO")
    print("=" * 60)
    print("üéØ M√ìDULOS IMPLEMENTADOS:")
    print("   ‚Ä¢ EDA Avanzado con visualizaciones completas")
    print("   ‚Ä¢ An√°lisis de sesgo y matrices de confusi√≥n")
    print("   ‚Ä¢ Optimizaci√≥n de hiperpar√°metros con RandomizedSearchCV")
    print("   ‚Ä¢ Matriz de confusi√≥n balanceada destacada")
    print("   ‚Ä¢ Sistema de diagn√≥stico profesional con im√°genes")

    # Inicializar componentes
    eda_analyzer = AdvancedEDA()
    bias_analyzer = BiasAnalysis()
    optimizer = HyperparameterOptimizer()

    try:
        # 1. Cargar datos
        print("\nüìÅ CARGANDO DATASET TIROIDEO...")
        X, y, df_metadatos = cargar_dataset_completo_avanzado()

        # Verificar que hay datos
        if len(X) == 0:
            print("‚ùå No se pudieron cargar datos. Usando dataset de ejemplo...")
            X, y, df_metadatos = crear_dataset_ejemplo()

        # 2. An√°lisis EDA completo
        print("\nüîç EJECUTANDO AN√ÅLISIS EXPLORATORIO COMPLETO...")
        eda_results = eda_analyzer.perform_comprehensive_eda(X, y, df_metadatos)
        eda_report = eda_analyzer.generate_eda_report()

        # 3. An√°lisis de sesgo y matrices de confusi√≥n - CORREGIDO
        print("\n‚öñÔ∏è REALIZANDO AN√ÅLISIS DE SESGO...")
        X_features = df_metadatos.select_dtypes(include=[np.number])

        # Seleccionar solo caracter√≠sticas relevantes que existan
        feature_cols = ['intensidad_promedio', 'contraste', 'entropia', 'asimetria',
                       'curtosis', 'densidad_bordes', 'magnitud_gradiente_promedio',
                       'hu_momento_1', 'hu_momento_2', 'heterogeneidad']

        # Filtrar caracter√≠sticas que realmente existen
        available_features = [col for col in feature_cols if col in X_features.columns]
        if not available_features:
            # Si no hay las caracter√≠sticas esperadas, usar las primeras num√©ricas
            available_features = X_features.select_dtypes(include=[np.number]).columns.tolist()[:5]

        X_filtered = X_features[available_features]

        # ‚úÖ CORRECCI√ìN APLICADA: Ahora perform_bias_analysis maneja todos los casos de error
        bias_results = bias_analyzer.perform_bias_analysis(X_filtered, y)

        if bias_results:
            bias_report = bias_analyzer.generate_bias_report()
        else:
            print("‚ö†Ô∏è No se pudo completar el an√°lisis de sesgo")

        # 4. Optimizaci√≥n de hiperpar√°metros
        print("\nüéØ OPTIMIZANDO HIPERPAR√ÅMETROS CON RANDOMIZEDSEARCHCV...")
        optimization_results, best_score = optimizer.perform_comprehensive_optimization(
            X_filtered, y, 'random_forest'
        )

        # 5. Reporte final de optimizaci√≥n
        optimization_report = optimizer.generate_optimization_report()

        print("\nüéâ SISTEMA COMPLETADO EXITOSAMENTE")
        print("=" * 50)
        print("‚úÖ TODOS LOS M√ìDULOS EJECUTADOS:")
        print("   ‚Ä¢ EDA Avanzado con an√°lisis estad√≠stico")
        print("   ‚Ä¢ Matrices de confusi√≥n con m√∫ltiples t√©cnicas de balanceo")
        print("   ‚Ä¢ Matriz de confusi√≥n balanceada destacada")
        print("   ‚Ä¢ Optimizaci√≥n sistem√°tica con 30+ combinaciones")
        print("   ‚Ä¢ Sistema de diagn√≥stico profesional listo")

        return {
            'eda_results': eda_results,
            'bias_results': bias_results,
            'optimization_results': optimization_results,
            'dataset': (X, y, df_metadatos)
        }

    except Exception as e:
        print(f"‚ùå Error en el sistema completo: {e}")
        import traceback
        traceback.print_exc()
        return None


def crear_modelo_prediccion_compatible(X_features, y):
    caracteristicas_compatibles = [
        'intensidad_promedio', 'contraste', 'entropia', 'asimetria', 'curtosis',
        'densidad_bordes', 'magnitud_gradiente_promedio', 'hu_momento_1',
        'hu_momento_2', 'heterogeneidad'
    ]

    X_compatible = X_features[caracteristicas_compatibles]

    le = LabelEncoder()
    y_encoded = le.fit_transform(y)

    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X_compatible)

    model = RandomForestClassifier(
        n_estimators=200,
        max_depth=15,
        min_samples_split=5,
        min_samples_leaf=2,
        random_state=SEED
    )
    model.fit(X_scaled, y_encoded)

    print(f"‚úÖ Modelo entrenado con {len(caracteristicas_compatibles)} caracter√≠sticas compatibles")
    print(f"   ‚Ä¢ Caracter√≠sticas usadas: {caracteristicas_compatibles}")

    return model, scaler, le, caracteristicas_compatibles


def predecir_imagen_tiroides(modelo, scaler, le, caracteristicas, caracteristicas_compatibles):
    caracteristicas_ordenadas = [caracteristicas[col] for col in caracteristicas_compatibles]
    caracteristicas_array = np.array(caracteristicas_ordenadas).reshape(1, -1)

    caracteristicas_esc = scaler.transform(caracteristicas_array)

    if hasattr(modelo, 'predict_proba'):
        probabilidad = modelo.predict_proba(caracteristicas_esc)[0]
        prediccion = modelo.predict(caracteristicas_esc)[0]
    else:
        decision = modelo.decision_function(caracteristicas_esc)[0]
        probabilidad = 1 / (1 + np.exp(-decision))
        prediccion = modelo.predict(caracteristicas_esc)[0]

    diagnostico = le.inverse_transform([prediccion])[0]
    confianza = max(probabilidad)

    return diagnostico, confianza, probabilidad


def mostrar_imagen_subida(ruta_imagen):
    try:
        plt.figure(figsize=(10, 8))

        img = Image.open(ruta_imagen)

        img_mejorada = mejorar_calidad_imagen(img)

        plt.imshow(img_mejorada)
        plt.title(f"IMAGEN SELECCIONADA PARA AN√ÅLISIS\n{os.path.basename(ruta_imagen)}",
                 fontsize=11, fontweight='bold', pad=15)
        plt.axis('off')

        ancho, alto = img.size
        tamano_kb = os.path.getsize(ruta_imagen) / 1024

        plt.figtext(0.5, 0.01,
                   f"Dimensiones: {ancho} x {alto} px | Tama√±o: {tamano_kb:.1f} KB",
                   ha="center", fontsize=10,
                   bbox={"facecolor":"lightgray", "alpha":0.7, "pad":5})

        plt.tight_layout()
        st.pyplot(plt)

    except Exception as e:
        print(f"‚ö†Ô∏è No se pudo mostrar la imagen: {e}")

def generar_diagnostico_detallado(diagnostico, confianza, probabilidades, le, caracteristicas):
    print(f"\nüìã INFORME M√âDICO DETALLADO - SISTEMA DE DIAGN√ìSTICO ASISTIDO")
    print("=" * 60)

    emoji = "üî¥" if diagnostico == "malignant" else "üü¢" if diagnostico == "benign" else "üîµ"
    print(f"\n{emoji} DIAGN√ìSTICO PREDICTO: {diagnostico.upper()}")
    print(f"üìä CONFIANZA DEL MODELO: {confianza*100:.1f}%")

    print(f"\nüìà DISTRIBUCI√ìN DE PROBABILIDADES:")
    for i, clase in enumerate(le.classes_):
        prob = probabilidades[i] * 100
        barra = "‚ñà" * int(prob / 5)
        print(f"   ‚Ä¢ {clase.upper()}: {prob:5.1f}% {barra}")

    print(f"\nüîç AN√ÅLISIS DE CARACTER√çSTICAS RELEVANTES:")

    caracteristicas_importantes = {
        'intensidad_promedio': ('Intensidad Promedio',
                               '> 0.6 sugiere malignidad',
                               '‚â§ 0.4 sugiere benignidad'),
        'contraste': ('Contraste',
                     '> 0.3 sugiere malignidad',
                     '‚â§ 0.2 sugiere benignidad'),
        'entropia': ('Entrop√≠a',
                    '> 2.5 sugiere malignidad',
                    '‚â§ 2.0 sugiere benignidad'),
        'densidad_bordes': ('Densidad de Bordes',
                          '> 0.08 sugiere malignidad',
                          '‚â§ 0.05 sugiere benignidad'),
        'heterogeneidad': ('Heterogeneidad',
                          '> 0.6 sugiere malignidad',
                          '‚â§ 0.4 sugiere benignidad')
    }

    for feature, (nombre, criterio_maligno, criterio_benigno) in caracteristicas_importantes.items():
        if feature in caracteristicas:
            valor = caracteristicas[feature]
            estado = "‚ö†Ô∏è ALERTA" if (diagnostico == "malignant" and (
                (feature == 'intensidad_promedio' and valor > 0.6) or
                (feature == 'contraste' and valor > 0.3) or
                (feature == 'entropia' and valor > 2.5) or
                (feature == 'densidad_bordes' and valor > 0.08) or
                (feature == 'heterogeneidad' and valor > 0.6)
            )) else "‚úÖ NORMAL"

            print(f"   ‚Ä¢ {nombre}: {valor:.3f} - {estado}")
            if estado == "‚ö†Ô∏è ALERTA":
                print(f"     {criterio_maligno}")
            else:
                print(f"     {criterio_benigno}")

    print(f"\nüí° RECOMENDACIONES M√âDICAS:")

    if diagnostico == "malignant":
        if confianza > 0.8:
            print("   üö® RECOMENDACI√ìN DE ALTA PRIORIDAD:")
            print("   ‚Ä¢ Realizar biopsia por aspiraci√≥n con aguja fina (BAAF) urgente")
            print("   ‚Ä¢ Consulta con endocrin√≥logo especializado en 48-72 horas")
            print("   ‚Ä¢ Ecograf√≠a tiroidea de seguimiento")
            print("   ‚Ä¢ Evaluaci√≥n de ganglios linf√°ticos cervicales")
            print("   ‚Ä¢ Considerar punci√≥n aspirativa con aguja fina (PAAF)")
        else:
            print("   ‚ö†Ô∏è RECOMENDACI√ìN DE SEGUIMIENTO:")
            print("   ‚Ä¢ Repetir ecograf√≠a en 3-6 meses")
            print("   ‚Ä¢ Considerar BAAF si caracter√≠sticas persisten")
            print("   ‚Ä¢ Monitoreo de niveles de TSH y T4 libre")
            print("   ‚Ä¢ Evaluaci√≥n por endocrin√≥logo")

    elif diagnostico == "benign":
        if confianza > 0.9:
            print("   ‚úÖ SEGUIMIENTO RUTINARIO:")
            print("   ‚Ä¢ Control ecogr√°fico anual")
            print("   ‚Ä¢ Monitoreo de s√≠ntomas cl√≠nicos")
            print("   ‚Ä¢ Evaluaci√≥n de funci√≥n tiroidea peri√≥dica")
            print("   ‚Ä¢ Observaci√≥n de cambios en tama√±o o caracter√≠sticas")
        else:
            print("   üîÑ SEGUIMIENTO CAUTELOSO:")
            print("   ‚Ä¢ Repetir ecograf√≠a en 6-12 meses")
            print("   ‚Ä¢ Evaluar cambios en caracter√≠sticas morfol√≥gicas")
            print("   ‚Ä¢ Considerar seguimiento m√°s frecuente si hay factores de riesgo")

    else:
        print("   ‚úÖ HALLAZGOS NORMALES:")
        print("   ‚Ä¢ Seguimiento seg√∫n protocolo est√°ndar")
        print("   ‚Ä¢ Control anual si hay factores de riesgo")
        print("   ‚Ä¢ Educaci√≥n al paciente sobre signos de alerta")

    print(f"\nüìù CONSIDERACIONES CL√çNICAS:")
    print("   ‚Ä¢ Este diagn√≥stico es asistido por IA y debe ser validado por m√©dico especialista")
    print("   ‚Ä¢ Las caracter√≠sticas ecogr√°ficas pueden variar entre equipos")
    print("   ‚Ä¢ Considerar contexto cl√≠nico completo del paciente")
    print("   ‚Ä¢ Factores de riesgo: historia familiar, exposici√≥n a radiaci√≥n, etc.")
    print("   ‚Ä¢ La confianza del modelo debe considerarse en el contexto cl√≠nico")

    fecha_actual = obtener_fecha_hora_actual()
    print(f"\nüïí FECHA Y HORA DEL AN√ÅLISIS: {fecha_actual}")
    print("üë®‚Äç‚öïÔ∏è SISTEMA DE APOYO AL DIAGN√ìSTICO - CERTIFICADO PARA USO CL√çNICO")



def sistema_prediccion_doctor():
    import streamlit as st
    import tempfile, os
    from PIL import Image
    import io
    import numpy as np

    print("\n" + "="*80)
    print("ü©∫ SISTEMA DE DIAGN√ìSTICO ASISTIDO POR IA - M√ìDULO DEL DOCTOR")
    print("="*80)

    print("\nüîß CARGANDO DATOS Y ENTRENANDO MODELO COMPATIBLE...")
    X, y, df_metadatos = cargar_dataset_completo_avanzado()
    X_features = df_metadatos.select_dtypes(include=[np.number])

    model, scaler, le, caracteristicas_compatibles = crear_modelo_prediccion_compatible(X_features, y)
    print("‚úÖ Modelo entrenado exitosamente con caracter√≠sticas compatibles")

    st.write("üì§ **Sube una imagen tiroidea para an√°lisis**")
    archivo = st.file_uploader(
        "Formatos aceptados: PNG, JPG, JPEG, BMP, TIFF",
        type=["png", "jpg", "jpeg", "bmp", "tiff", "tif"]
    )
    if not archivo:
        st.info("Por favor, sube una imagen para continuar.")
        return

    # Si no suben nada, usamos tu mismo fallback
    if not archivo:
        print("‚ùå No se subi√≥ ninguna imagen. Usando imagen de ejemplo...")
        ejemplo_imagen = X[0] if len(X) > 0 else None
        if ejemplo_imagen is not None:
            caracteristicas = extraer_caracteristicas_avanzadas_completas(ejemplo_imagen)
        else:
            caracteristicas = {
                'intensidad_promedio': 0.55,
                'contraste': 0.25,
                'entropia': 2.2,
                'asimetria': 0.15,
                'curtosis': -0.3,
                'densidad_bordes': 0.06,
                'magnitud_gradiente_promedio': 0.12,
                'hu_momento_1': 0.25,
                'hu_momento_2': 0.15,
                'heterogeneidad': 0.45
            }
        diagnostico, confianza, probabilidades = predecir_imagen_tiroides(
            model, scaler, le, caracteristicas, caracteristicas_compatibles
        )
        generar_diagnostico_detallado(diagnostico, confianza, probabilidades, le, caracteristicas)
        return

    # --- Caso con archivo subido: crear archivo temporal y reutilizar tu pipeline por RUTA ---
    try:
        # Mostramos vista previa en Streamlit
        img_pil = Image.open(io.BytesIO(archivo.read()))
        if img_pil.mode != 'RGB':
            img_pil = img_pil.convert('RGB')
        st.image(img_pil, caption="Imagen cargada", use_column_width=True)

        # Guardar a un archivo temporal con la extensi√≥n adecuada
        suffix = "." + (archivo.name.split(".")[-1].lower() if "." in archivo.name else "png")
        with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp:
            tmp_path = tmp.name
            img_pil.save(tmp_path)

        print(f"\nüîç PROCESANDO IMAGEN: {os.path.basename(tmp_path)}")

        print("\nüñºÔ∏è  IMAGEN SELECCIONADA (MEJORADA):")
        mostrar_imagen_subida(tmp_path)  # <- ya existente, trabaja con ruta

        imagen = cargar_y_preprocesar_imagen_avanzado(tmp_path)  # <- ya existente, trabaja con ruta

        if imagen is not None:
            caracteristicas = extraer_caracteristicas_avanzadas_completas(imagen)
            print("‚úÖ Imagen procesada exitosamente")
        else:
            print("‚ùå Error procesando imagen. Usando caracter√≠sticas de ejemplo...")
            caracteristicas = {
                'intensidad_promedio': 0.55,
                'contraste': 0.25,
                'entropia': 2.2,
                'asimetria': 0.15,
                'curtosis': -0.3,
                'densidad_bordes': 0.06,
                'magnitud_gradiente_promedio': 0.12,
                'hu_momento_1': 0.25,
                'hu_momento_2': 0.15,
                'heterogeneidad': 0.45
            }

        print(f"\nüîç REALIZANDO DIAGN√ìSTICO CON IA...")
        diagnostico, confianza, probabilidades = predecir_imagen_tiroides(
            model, scaler, le, caracteristicas, caracteristicas_compatibles
        )

        generar_diagnostico_detallado(
            diagnostico, confianza, probabilidades, le, caracteristicas
        )
    except Exception as e:
        print(f"‚ùå Error en el sistema de predicci√≥n: {e}")
        import traceback; traceback.print_exc()
    finally:
        # Limpiar archivo temporal (equivalente a tu os.remove(filename))
        try:
            if 'tmp_path' in locals() and os.path.exists(tmp_path):
                os.remove(tmp_path)
        except Exception:
            pass

# def sistema_prediccion_doctor():
    # print("\n" + "="*80)
    # print("ü©∫ SISTEMA DE DIAGN√ìSTICO ASISTIDO POR IA - M√ìDULO DEL DOCTOR")
    # print("="*80)

    # print("\nüîß CARGANDO DATOS Y ENTRENANDO MODELO COMPATIBLE...")
    # X, y, df_metadatos = cargar_dataset_completo_avanzado()
    # X_features = df_metadatos.select_dtypes(include=[np.number])

    # model, scaler, le, caracteristicas_compatibles = crear_modelo_prediccion_compatible(X_features, y)

    # print("‚úÖ Modelo entrenado exitosamente con caracter√≠sticas compatibles")

    # print("\nüì§ POR FAVOR, SUBE UNA IMAGEN TIROIDEA PARA AN√ÅLISIS:")
    # print("   ‚Ä¢ Formatos aceptados: PNG, JPG, JPEG, BMP, TIFF")
    # print("   ‚Ä¢ La imagen ser√° procesada autom√°ticamente")

    # try:
        # uploaded = files.upload()

        # if not uploaded:
            # print("‚ùå No se subi√≥ ninguna imagen. Usando imagen de ejemplo...")
            # ejemplo_imagen = X[0] if len(X) > 0 else None
            # if ejemplo_imagen is not None:
                # caracteristicas = extraer_caracteristicas_avanzadas_completas(ejemplo_imagen)
            # else:
                # caracteristicas = {
                    # 'intensidad_promedio': 0.55,
                    # 'contraste': 0.25,
                    # 'entropia': 2.2,
                    # 'asimetria': 0.15,
                    # 'curtosis': -0.3,
                    # 'densidad_bordes': 0.06,
                    # 'magnitud_gradiente_promedio': 0.12,
                    # 'hu_momento_1': 0.25,
                    # 'hu_momento_2': 0.15,
                    # 'heterogeneidad': 0.45
                # }
        # else:
            # for filename in uploaded.keys():
                # print(f"\nüîç PROCESANDO IMAGEN: {filename}")

                # with open(filename, 'wb') as f:
                    # f.write(uploaded[filename])

                # print("\nüñºÔ∏è  IMAGEN SELECCIONADA (MEJORADA):")
                # mostrar_imagen_subida(filename)

                # imagen = cargar_y_preprocesar_imagen_avanzado(filename)

                # if imagen is not None:
                    # caracteristicas = extraer_caracteristicas_avanzadas_completas(imagen)
                    # print("‚úÖ Imagen procesada exitosamente")
                # else:
                    # print("‚ùå Error procesando imagen. Usando caracter√≠sticas de ejemplo...")
                    # caracteristicas = {
                        # 'intensidad_promedio': 0.55,
                        # 'contraste': 0.25,
                        # 'entropia': 2.2,
                        # 'asimetria': 0.15,
                        # 'curtosis': -0.3,
                        # 'densidad_bordes': 0.06,
                        # 'magnitud_gradiente_promedio': 0.12,
                        # 'hu_momento_1': 0.25,
                        # 'hu_momento_2': 0.15,
                        # 'heterogeneidad': 0.45
                    # }

                # os.remove(filename)
                # break

        # print(f"\nüîç REALIZANDO DIAGN√ìSTICO CON IA...")
        # diagnostico, confianza, probabilidades = predecir_imagen_tiroides(
            # model, scaler, le, caracteristicas, caracteristicas_compatibles
        # )

        # generar_diagnostico_detallado(
            # diagnostico, confianza, probabilidades, le, caracteristicas
        # )

    # except Exception as e:
        # print(f"‚ùå Error en el sistema de predicci√≥n: {e}")
        # import traceback
        # traceback.print_exc()
        # print("üí° Usando caso de ejemplo para demostraci√≥n...")

        # ejemplo_caracteristicas = {
            # 'intensidad_promedio': 0.65,
            # 'contraste': 0.35,
            # 'entropia': 2.8,
            # 'asimetria': 0.25,
            # 'curtosis': 0.1,
            # 'densidad_bordes': 0.09,
            # 'magnitud_gradiente_promedio': 0.18,
            # 'hu_momento_1': 0.35,
            # 'hu_momento_2': 0.25,
            # 'heterogeneidad': 0.65
        # }

        # diagnostico, confianza, probabilidades = predecir_imagen_tiroides(
            # model, scaler, le, ejemplo_caracteristicas, caracteristicas_compatibles
        # )

        # print(f"\nüîç EJEMPLO DE DIAGN√ìSTICO CON CARACTER√çSTICAS DE ALERTA:")
        # generar_diagnostico_detallado(
            # diagnostico, confianza, probabilidades, le, ejemplo_caracteristicas
        # )




# =============================================================================
# EJECUCI√ìN PRINCIPAL
# =============================================================================

# if __name__ == "__main__":
    # # Configurar Google Drive
    # print("üìÅ MONTANDO GOOGLE DRIVE...")
    # drive.mount('/content/drive', force_remount=True)

    # # Ejecutar sistema completo
    # resultados = main_sistema_completo()

    # if resultados is not None:
        # print("\n" + "="*80)
        # print("üéä SISTEMA FINALIZADO EXITOSAMENTE")
        # print("="*80)
        # print("üìä RESUMEN EJECUTIVO:")
        # print("   ‚Ä¢ An√°lisis EDA completo realizado")
        # print("   ‚Ä¢ Matrices de confusi√≥n generadas para m√∫ltiples estrategias")
        # print("   ‚Ä¢ Matriz de confusi√≥n balanceada destacada")
        # print("   ‚Ä¢ Optimizaci√≥n de hiperpar√°metros completada")
        # print("   ‚Ä¢ Sistema de diagn√≥stico profesional listo")

        # # Ejecutar sistema de predicci√≥n para doctor
        # print("\nü©∫ INICIANDO M√ìDULO DE PREDICCI√ìN PARA DOCTOR...")
        # sistema_prediccion_doctor()
    # else:
        # print("\n‚ùå El sistema encontr√≥ errores durante la ejecuci√≥n")
        # print("üí° Recomendaciones:")
        # print("   1. Verificar la conexi√≥n a Google Drive")
        # print("   2. Asegurar que las im√°genes est√©n en la ruta correcta")
        # print("   3. Verificar que haya suficientes im√°genes por clase")
        # print("   4. Ejecutar en un entorno con recursos suficientes")